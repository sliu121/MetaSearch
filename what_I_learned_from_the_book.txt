What is a metasearch engine? There are too many explaination you can find on Internet. Here is what I think. Metasearch is not a 'search engine'! It's a 'thief'! It 'steals' the result
from other search engines like Google, Bing, etc. But metasearch will analyze all the result it got, and give user the best result. So, metasearch is a search engine of search engine,
and best part of it is drawing irrelevant and redundancy results by using some algorithm. From the Ch4 intro part, we could know, if we want to establish a connection between 
a metasearch engine and a search engine, there are 2 things we should do: connector and result extractor. 

Before talking about these two, let's talk about HTTP form tag first. In an HTTP form of search engine, there contains an input control of text type( creat an input field), an action 
attribute( specifies the name and location of search engine server to which the user query will be sent when the form is submitted), a submit input control ( define the name of 
the submit button), and a method attribute (specifies the HTTP method for sending the query to the search engine server). In method, there are 2 types we need to understand, 
get and post. Get has a limited size of user input and characters user can use, and also permits only the retrieval of information from a remote server, and foremost it is not secure 
for transmitting important data over the Internet. In contrast, there is no limit size or character on post, and post may request storing and updating data in addition to retrieving 
information. It is not easy to understand right? Let's talk it more noob way, my way. Get likes 'SELECT' in SQL, and post likes 'UPDATE', one is for getting information from server and
the other one is for sending information to update/ create resource for server. Not only these reasons decide we use get more frequently, also, search engines users don't need 
high-level secure and their inputs are short; get request can be bookmarked, for those same search input, server could give answer more quickly. 

However, GET is not always the best choice. When we use a search engine, what we do? first input what we want to search, then click submit, finally get what we want. During this 
operation, after submitting what we want to search, there will generate a new URL first and it includes "action +?+ input w/ other default info ". Action is the server, input is characters 
you inputted, and between them, there is a ?. Or we could say that first ? in URL separates action and GET request message. Hold on, did you find what I just said above this line. 
Yes, GET request message =(likely) input w/ other default info, And in this message all the characters you inputted will be shown. Now you know there are some problem, how 
about my username and password? So, thats why we need POST. In POST request message, our inputs  length and type will be shown instead of content. 

Now, since we have learned how HTTP works, then let's talk about connector. Why we need connector? Because metasearch engine usually cannot utilize a browser to transmit 
the HTTP request message and recieve the HTTP response message. Now how to generate a connector? As we already talked above, a search engine need send a request 
message to the server, so what is the format or info we need for this request? Here it is: name and location of the search engine server, the HTTP request method, The name of each
input control in the search form (except the submit type) and its value if exists. Since we are CS students or computer workers, we need to 'make' an automatic search connector,
which could find and get the info from pages automatically. First step, how to find? we know that all these info are in <form>..</form>, if there only contains one input control 
and it is for search, that is really easy. However, most time there are multiple input controls in one web page, and we have to recognize the correct one. There are two solutions we
could use. First one, checking a) if there is a text input field in <form> and b) at least one of these keywords such as " search,” “query” or “find” appears, either in the form tag or 
in the text immediately preceding or following the form tag. The other one, using a decision tree(details are not including here). However, in reality, we could not guess which 
search engine a user looking for. So, here is our team job, find all the possible search engines for users. 
 
 After finding these info we need, next step is extracting the search form. In most cases, it is very easy since we could find name and location of search engine server in action 
 attribute, HTTP request method from method attribute, and the name of each input control with its value from the name and value attributes of the input tag. However, in some
 cases, we cannot find the name and location of search engine server easily since there is a relative URL or no-URL at all in action, then we need to construct a URL for HTTP 
 request message. For relative URL, there are two types about it. One is without '/' and the other one is. For the former one, we need to combine it with relative or current URL
 we have and then construct a new one. For the latter case, all we need to do is combine with domain name of the current URL. 

 Ok, now we have already know how to construct a connection with search engines, we should look how to extract result from those search engine. As we all know, after 
 receiving query from user, server will give a lot of response pages and we call those pages as Search result records (SRRs). However, in those response pages, there are some 
 unnecessary items such as advertisements or something user not looking for, then metasearch engine's job is extracting the correct SRRs and sorting them for users. We call 
 this extracion as extraction wrapper. In theses days, we have already had 2 main method. One is, treating every page as a new page and analyzing them every time, even 
 some of them may already be analyzed before. For every students, we should know that there are many disadvantages in this method, and for CS students, we know how 
 to call this method, brute force, which equals to high time cost. As a result, we would not talk this one here. We will talk the other one, generating the extraction rules from 
 sample response pages and apply the rules to perform extraction for new response pages returned from the same search engine. This method just like machine learning type,
 analyze samples and give the prediction for a new case. And we call this extraction method as wrapper-based approach. And according how it works, we have three different 
 types of extraction. Manual, which asks a high-level skills developer to analyze from HTML source files of samples and identify the patterns or rules for extracting SRRs; 
 semiautomatic, which need human manually mark correct SRRs and machine will generate a rule according to human work. Those 2 methods are not good enough. Semiautomatic
is  not good, cause we are CS students, we create computer because we dont want to work, also manual really need a strong- experienced developer for identification. So, 
as you already guess, we gonna talk about the final method, automatic approach which we mentioned above several times. But before to this real topic, we should learn some
old-time machine, Semiautomatic approach.
